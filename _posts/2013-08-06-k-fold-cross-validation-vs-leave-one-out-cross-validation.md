---
id: 70
title: K-Fold Cross Validation VS Leave-One-Out Cross Validation
date: 2013-08-06T13:17:20+00:00
author: lo
layout: post
guid: http://www.chioka.in/?p=70
permalink: /k-fold-cross-validation-vs-leave-one-out-cross-validation/
categories:
  - Machine Learning
  - Reading List
tags:
  - Cross Validation
  - Explain To Me
  - Machine Learning
  - Reading List
---
We use stratified K-fold cross-validation a lot nowadays, but wonder where it started to got prominent (as an idea)?

This paper, [A Study of CrossValidation and Bootstrap for Accuracy Estimation and Model Selection](http://robotics.stanford.edu/~ronnyk/accEst.pdf),Â by Rohavi 1995 explains how when in the older past, researchers tend to use Leave-one-out cross validation (wiki please), and how K-fold (k=10/20, though I use 5 generally) actually yield better results.

&nbsp;